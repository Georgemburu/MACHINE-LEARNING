{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "(SPANISH to ENGLISH )translation revision 1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Georgemburu/MACHINE-LEARNING/blob/master/(SPANISH_to_ENGLISH_)translation_revision_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSNawEHXOfKl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# MACHINE LEARNING TRANSLATION WITH ATTENTION"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQXTB_-OQJYT",
        "colab_type": "code",
        "outputId": "710e03e3-fecf-4e99-9116-0efbba0dee06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yPIizXOMP566",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, print_function, division,unicode_literals\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import unicodedata\n",
        "import re\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JQP32IRP6Vo",
        "colab_type": "code",
        "outputId": "21dabd95-c4eb-4570-bd2b-4a77b9003c39",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "path_to_zip = tf.keras.utils.get_file(\n",
        "    'spa-eng.zip',\n",
        "    origin = 'http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip',\n",
        "    extract=True\n",
        ")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n",
            "2646016/2638744 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNxl6ragP6xI",
        "colab_type": "code",
        "outputId": "e10b53d0-3fda-457d-8c3e-864a280959e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "os.listdir(os.path.dirname(path_to_zip)+'/spa-eng')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['spa.txt', '_about.txt']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJ8q1NrgP6-W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path_to_file = os.path.join(os.path.dirname(path_to_zip),'spa-eng/spa.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpgHV7IcP66A",
        "colab_type": "code",
        "outputId": "cd12a6b0-c817-4c9e-cd78-e947092dabae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        }
      },
      "source": [
        "pd.read_csv(path_to_file,delimiter='\\t')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Go.</th>\n",
              "      <th>Ve.</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Go.</td>\n",
              "      <td>Vete.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Go.</td>\n",
              "      <td>Vaya.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Go.</td>\n",
              "      <td>Váyase.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Hi.</td>\n",
              "      <td>Hola.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Run!</td>\n",
              "      <td>¡Corre!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>118958</th>\n",
              "      <td>There are four main causes of alcohol-related ...</td>\n",
              "      <td>Hay cuatro causas principales de muertes relac...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>118959</th>\n",
              "      <td>There are mothers and fathers who will lie awa...</td>\n",
              "      <td>Hay madres y padres que se quedan despiertos d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>118960</th>\n",
              "      <td>A carbon footprint is the amount of carbon dio...</td>\n",
              "      <td>Una huella de carbono es la cantidad de contam...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>118961</th>\n",
              "      <td>Since there are usually multiple websites on a...</td>\n",
              "      <td>Como suele haber varias páginas web sobre cual...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>118962</th>\n",
              "      <td>If you want to sound like a native speaker, yo...</td>\n",
              "      <td>Si quieres sonar como un hablante nativo, debe...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>118963 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                      Go.                                                Ve.\n",
              "0                                                     Go.                                              Vete.\n",
              "1                                                     Go.                                              Vaya.\n",
              "2                                                     Go.                                            Váyase.\n",
              "3                                                     Hi.                                              Hola.\n",
              "4                                                    Run!                                            ¡Corre!\n",
              "...                                                   ...                                                ...\n",
              "118958  There are four main causes of alcohol-related ...  Hay cuatro causas principales de muertes relac...\n",
              "118959  There are mothers and fathers who will lie awa...  Hay madres y padres que se quedan despiertos d...\n",
              "118960  A carbon footprint is the amount of carbon dio...  Una huella de carbono es la cantidad de contam...\n",
              "118961  Since there are usually multiple websites on a...  Como suele haber varias páginas web sobre cual...\n",
              "118962  If you want to sound like a native speaker, yo...  Si quieres sonar como un hablante nativo, debe...\n",
              "\n",
              "[118963 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uoRJvi-PP6sS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def unicode_to_ascii(s):\n",
        "  return ''.join(c for c in unicodedata.normalize('NFD',s)\n",
        "    if unicodedata.category(c)!='Mn')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0OitVBzS2nF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess_sentence(sentence):\n",
        "  s = unicode_to_ascii(sentence.lower().strip())\n",
        "  s = re.sub(r\"([?.!,¿])\", r\" \\1 \", s)\n",
        "  s = re.sub(r'[\" \"]+', \" \", s)\n",
        "\n",
        "  # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
        "  s = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", s)\n",
        "\n",
        "  s = s.rstrip().strip()\n",
        "\n",
        "  # adding a start and an end token to the sentence\n",
        "  # so that the model know when to start and stop predicting.\n",
        "  s = '<start> ' + s + ' <end>'\n",
        "  return s\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1eF0ViibTWii",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_dataset(filepath,num_examples=None):\n",
        "  lines = open(filepath,'r',encoding ='utf-8').read().strip().split('\\n')\n",
        "  # data = []\n",
        "  inp = []\n",
        "  targ = []\n",
        "  for line in lines:\n",
        "    inp_targ_arr =  line.split('\\t')\n",
        "    inp.append(preprocess_sentence(inp_targ_arr[1]))\n",
        "    targ.append(preprocess_sentence(inp_targ_arr[0]))\n",
        "    inp_targ_arr = None\n",
        "  return inp,targ\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1L8LDtqP6P-",
        "colab_type": "code",
        "outputId": "477d9513-070c-41e2-eab0-236a3cde17d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "inp,targ = load_dataset(path_to_file,10)\n",
        "print(inp[-1])\n",
        "print(targ[-1])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<start> si quieres sonar como un hablante nativo , debes estar dispuesto a practicar diciendo la misma frase una y otra vez de la misma manera en que un musico de banjo practica el mismo fraseo una y otra vez hasta que lo puedan tocar correctamente y en el tiempo esperado . <end>\n",
            "<start> if you want to sound like a native speaker , you must be willing to practice saying the same sentence over and over in the same way that banjo players practice the same phrase over and over until they can play it correctly and at the desired tempo . <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JgWoPazkV9B5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tokenize\n",
        "def tokenize(lang):\n",
        "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "  lang_tokenizer.fit_on_texts(lang)\n",
        "  tensors = lang_tokenizer.texts_to_sequences(lang)\n",
        "  # Padding\n",
        "  tensors = tf.keras.preprocessing.sequence.pad_sequences(tensors,padding='post')\n",
        "  return tensors, lang_tokenizer\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OekVDqkhXLOT",
        "colab_type": "code",
        "outputId": "ed20c959-c65e-46bc-f934-557d9c276c53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "inp_tensor, inp_word_index = tokenize(inp)\n",
        "inp_tensor[0]"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  1, 364,   3,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBv_gWzXXk2r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def max_len(tensor):\n",
        "  return max(len(t) for t in tensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Z7esR7xX9i1",
        "colab_type": "code",
        "outputId": "df0fc350-edad-41b9-dd08-414badd6095d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "input_tensor_max_len = max_len(inp_tensor)\n",
        "input_tensor_max_len"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "53"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBVVcoVlYKrS",
        "colab_type": "code",
        "outputId": "1710ba2e-4d79-4590-872e-c57d9fdf0991",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "targ_tensor, targ_word_index = tokenize(targ)\n",
        "targ_tensor[0]"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 1, 49,  3,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
              "      dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqNxpYRtYLJB",
        "colab_type": "code",
        "outputId": "e510049d-3d78-475b-ea8f-8154fcb533e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "targ_tensor_max_len = max_len(targ_tensor)\n",
        "targ_tensor_max_len"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "51"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFR6kItOYbPY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create dataset\n",
        "BUFFER_SIZE = 3000\n",
        "BATCH_SIZE = 64\n",
        "dataset = tf.data.Dataset.from_tensor_slices((inp_tensor,targ_tensor)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(64, drop_remainder=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QEHTbjeVZDki",
        "colab_type": "code",
        "outputId": "f0474110-d779-4b1c-9c24-e812c6b3bbbd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        }
      },
      "source": [
        "for sample_inp, sample_targ in dataset.take(1):\n",
        "  print(sample_inp, sample_targ)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[    1  7609   633 ...     0     0     0]\n",
            " [    1    12     4 ...     0     0     0]\n",
            " [    1    40  4580 ...     0     0     0]\n",
            " ...\n",
            " [    1    38    98 ...     0     0     0]\n",
            " [    1 14770    57 ...     0     0     0]\n",
            " [    1  3675    20 ...     0     0     0]], shape=(64, 53), dtype=int32) tf.Tensor(\n",
            "[[  1 678  46 ...   0   0   0]\n",
            " [  1  29  15 ...   0   0   0]\n",
            " [  1   4  40 ...   0   0   0]\n",
            " ...\n",
            " [  1   4 101 ...   0   0   0]\n",
            " [  1 222  23 ...   0   0   0]\n",
            " [  1 355  14 ...   0   0   0]], shape=(64, 51), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhNKDGBnZa7a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create the encoder\n",
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self,vocab_size,embedding_dim,enc_units,batch_size):\n",
        "    super(Encoder,self).__init__()\n",
        "    self.enc_units = enc_units\n",
        "    self.batch_size = batch_size\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size,embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(enc_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    \n",
        "  def call(self,x,hidden):\n",
        "    x = self.embedding(x)\n",
        "    output,state = self.gru(x, initial_state=hidden)\n",
        "    return output, state\n",
        "\n",
        "  def initialize_hidden_state(self):\n",
        "    return tf.zeros((self.batch_size,self.enc_units))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6taR2OzMZ7-Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Attention layer\n",
        "class AttentionLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self,units):\n",
        "    super(AttentionLayer,self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.FC = tf.keras.layers.Dense(1)\n",
        "    \n",
        "  def call(self, query, values):\n",
        "    # hidden_with_time_axis\n",
        "    hidden_with_time_axis = tf.expand_dims(query,1)\n",
        "    # score\n",
        "    score = self.FC(\n",
        "        tf.tanh(self.W1(values)+ self.W2(hidden_with_time_axis))\n",
        "    )\n",
        "    # attention weights\n",
        "    attention_weights = tf.nn.softmax(score,axis=1)\n",
        "\n",
        "    # context vector\n",
        "    # attention_weights * values\n",
        "    context_vector = attention_weights * values\n",
        "    context_vector = tf.reduce_sum(context_vector,axis=1)\n",
        "    \n",
        "    # return\n",
        "    return context_vector, attention_weights\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Grz9hugIZbh_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Decoder\n",
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self,vocab_size,embedding_dims,dec_units,batch_size):\n",
        "    super(Decoder,self).__init__()\n",
        "    self.dec_units = dec_units\n",
        "    self.batch_size = batch_size\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size,embedding_dims)\n",
        "    self.gru = tf.keras.layers.GRU(dec_units,\n",
        "                                   return_state=True,\n",
        "                                   return_sequences = True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "    # Attention\n",
        "    self.attention = AttentionLayer(self.dec_units)\n",
        "\n",
        "  def call(self,x,hidden,enc_output):\n",
        "    # pass through attention\n",
        "    context_vector, attention_weights = self.attention(hidden,enc_output)\n",
        "\n",
        "    # pass x through embedding\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    # concat context_vector with x\n",
        "    x = tf.concat([tf.expand_dims(context_vector,axis=1),x],axis=-1)\n",
        "    # Passing concatenated x to gru\n",
        "    output, state = self.gru(x)\n",
        "\n",
        "    # reshape output\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "    # output shape == (batch_sze, vocab)\n",
        "\n",
        "    x = self.fc(output)\n",
        "\n",
        "    return x, state, attention_weights\n",
        "\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ksiBQHSLZb5k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Optimizer and Loss function\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True,\n",
        "    reduction='none'\n",
        ")\n",
        "def loss_function(real,pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real,0))\n",
        "\n",
        "  loss_ = loss_object(real,pred)\n",
        "\n",
        "  mask = tf.cast(mask,dtype=loss_.dtype)\n",
        "\n",
        "  loss_ *= mask\n",
        "  return tf.reduce_mean(loss_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcHGRXzQoNDa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "####################\n",
        "# Split data\n",
        "val_data_size = -20\n",
        "inp_tensor_train = inp_tensor[:val_data_size]\n",
        "targ_tensor_train = targ_tensor[:val_data_size]\n",
        "# VARIABLES\n",
        "##################\n",
        "BUFFER_SIZE = len(inp_tensor_train)\n",
        "BATCH_SIZE = 64\n",
        "steps_per_epochs = BUFFER_SIZE//BATCH_SIZE\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "vocab_inp_size = len(inp_word_index.word_index)+1\n",
        "vocab_tar_size = len(targ_word_index.word_index)+1\n",
        "\n",
        "\n",
        "# Create dataset\n",
        "dataset = tf.data.Dataset.from_tensor_slices((inp_tensor_train,targ_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE,drop_remainder=True)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Wh6hpFMprhU",
        "colab_type": "code",
        "outputId": "eb736263-ef0e-43d9-8a21-2ced42d610ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        }
      },
      "source": [
        "for sample_inp, sample_targ in dataset.take(1):\n",
        "  print(sample_inp, sample_targ)\n",
        "# Initialize the decoder encoder\n",
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "# sample input\n",
        "sample_hidden = encoder.initialize_hidden_state()\n",
        "sample_output, sample_hidden = encoder(sample_inp, sample_hidden)\n",
        "\n",
        "attention_layer = AttentionLayer(10)\n",
        "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
        "\n",
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
        "                                      sample_hidden, sample_output)\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[   1    8   17 ...    0    0    0]\n",
            " [   1   12  454 ...    0    0    0]\n",
            " [   1   12  196 ...    0    0    0]\n",
            " ...\n",
            " [   1    9  822 ...    0    0    0]\n",
            " [   1    6  144 ...    0    0    0]\n",
            " [   1   47 9148 ...    0    0    0]], shape=(64, 53), dtype=int32) tf.Tensor(\n",
            "[[  1   4  30 ...   0   0   0]\n",
            " [  1  55 123 ...   0   0   0]\n",
            " [  1  21   7 ...   0   0   0]\n",
            " ...\n",
            " [  1   8 709 ...   0   0   0]\n",
            " [  1   5 152 ...   0   0   0]\n",
            " [  1   4  39 ...   0   0   0]], shape=(64, 51), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vuknYxWncB7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Checkpoints\n",
        "checkpoints_dir = './training_checkpoints'\n",
        "checkpoints_prefix = os.path.join(checkpoints_dir, 'ckpts')\n",
        "checkpoint = tf.train.Checkpoint(\n",
        "    optimizer=optimizer,\n",
        "    encoder=encoder,\n",
        "    decoder=decoder\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8yTBT5i2rwe3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###############################\n",
        "# TRAIN\n",
        "@tf.function\n",
        "def train_step(inp,targ,enc_hidden):\n",
        "  loss = 0\n",
        "  with tf.GradientTape() as tape:\n",
        "    enc_output, enc_hidden = encoder(inp,enc_hidden)\n",
        "    dec_hidden = enc_hidden\n",
        "    dec_input = tf.expand_dims([targ_word_index.word_index['<start>']]*BATCH_SIZE, axis=1)\n",
        "\n",
        "    # Use Teacher Forcing\n",
        "    for t in range(1,targ.shape[1]):\n",
        "      # passing enc_output to the decoder\n",
        "      predictions, dec_hidden,_ = decoder(dec_input,dec_hidden,enc_output)\n",
        "      loss += loss_function(targ[:,t], predictions)\n",
        "      # using teacher forcing\n",
        "      dec_input = tf.expand_dims(targ[:,t],axis=1)\n",
        "\n",
        "    batch_loss = (loss/int(targ.shape[1]))\n",
        "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "    gradients = tape.gradient(loss, variables)\n",
        "    optimizer.apply_gradients(zip(gradients, variables))\n",
        "    return batch_loss\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxzDybATrw0n",
        "colab_type": "code",
        "outputId": "9ec5e870-d578-4e0d-f38c-281571a69aad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "EPOCHS = 10\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "  enc_hidden = encoder.initialize_hidden_state()\n",
        "  total_loss = 0\n",
        "\n",
        "  for(batch,(inp,targ)) in enumerate(dataset.take(steps_per_epochs)):\n",
        "    batch_loss = train_step(inp,targ,enc_hidden)\n",
        "    total_loss += batch_loss\n",
        "\n",
        "    if(batch % 100 == 0):\n",
        "      print('Epoch {} Batch {} Loss {:.4f}'.format(\n",
        "          epoch +1,\n",
        "          batch,\n",
        "          batch_loss.numpy()\n",
        "      ))\n",
        "      # saving (checkpoint) the model every 2 epochs\n",
        "      if(epoch+1)%2 == 0:\n",
        "        checkpoint.save(file_prefix=checkpoints_prefix)\n",
        "      \n",
        "      print('Epoch {} Loss {:.4f}'.format(epoch+1,\n",
        "                                          total_loss / steps_per_epochs))\n",
        "      print('Time taken for 1 epoch {} sec\\n'.format(\n",
        "          time.time() - start\n",
        "      ))\n",
        "      "
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 1.6650\n",
            "Epoch 1 Loss 0.0009\n",
            "Time taken for 1 epoch 46.687591552734375 sec\n",
            "\n",
            "Epoch 1 Batch 100 Loss 0.8423\n",
            "Epoch 1 Loss 0.0524\n",
            "Time taken for 1 epoch 89.83947205543518 sec\n",
            "\n",
            "Epoch 1 Batch 200 Loss 0.7311\n",
            "Epoch 1 Loss 0.0971\n",
            "Time taken for 1 epoch 133.11894607543945 sec\n",
            "\n",
            "Epoch 1 Batch 300 Loss 0.7558\n",
            "Epoch 1 Loss 0.1387\n",
            "Time taken for 1 epoch 176.212660074234 sec\n",
            "\n",
            "Epoch 1 Batch 400 Loss 0.7372\n",
            "Epoch 1 Loss 0.1778\n",
            "Time taken for 1 epoch 219.3749725818634 sec\n",
            "\n",
            "Epoch 1 Batch 500 Loss 0.6097\n",
            "Epoch 1 Loss 0.2146\n",
            "Time taken for 1 epoch 262.59108424186707 sec\n",
            "\n",
            "Epoch 1 Batch 600 Loss 0.6466\n",
            "Epoch 1 Loss 0.2488\n",
            "Time taken for 1 epoch 305.8367009162903 sec\n",
            "\n",
            "Epoch 1 Batch 700 Loss 0.5608\n",
            "Epoch 1 Loss 0.2813\n",
            "Time taken for 1 epoch 348.96155834198 sec\n",
            "\n",
            "Epoch 1 Batch 800 Loss 0.5991\n",
            "Epoch 1 Loss 0.3117\n",
            "Time taken for 1 epoch 392.02862524986267 sec\n",
            "\n",
            "Epoch 1 Batch 900 Loss 0.5693\n",
            "Epoch 1 Loss 0.3405\n",
            "Time taken for 1 epoch 435.3520772457123 sec\n",
            "\n",
            "Epoch 1 Batch 1000 Loss 0.4739\n",
            "Epoch 1 Loss 0.3676\n",
            "Time taken for 1 epoch 478.65175890922546 sec\n",
            "\n",
            "Epoch 1 Batch 1100 Loss 0.4579\n",
            "Epoch 1 Loss 0.3932\n",
            "Time taken for 1 epoch 521.8527009487152 sec\n",
            "\n",
            "Epoch 1 Batch 1200 Loss 0.4904\n",
            "Epoch 1 Loss 0.4174\n",
            "Time taken for 1 epoch 565.1886422634125 sec\n",
            "\n",
            "Epoch 1 Batch 1300 Loss 0.4396\n",
            "Epoch 1 Loss 0.4411\n",
            "Time taken for 1 epoch 608.3010816574097 sec\n",
            "\n",
            "Epoch 1 Batch 1400 Loss 0.4373\n",
            "Epoch 1 Loss 0.4629\n",
            "Time taken for 1 epoch 651.4784853458405 sec\n",
            "\n",
            "Epoch 1 Batch 1500 Loss 0.3562\n",
            "Epoch 1 Loss 0.4842\n",
            "Time taken for 1 epoch 694.6832559108734 sec\n",
            "\n",
            "Epoch 1 Batch 1600 Loss 0.4000\n",
            "Epoch 1 Loss 0.5042\n",
            "Time taken for 1 epoch 737.943689584732 sec\n",
            "\n",
            "Epoch 1 Batch 1700 Loss 0.3271\n",
            "Epoch 1 Loss 0.5239\n",
            "Time taken for 1 epoch 781.1303329467773 sec\n",
            "\n",
            "Epoch 1 Batch 1800 Loss 0.3577\n",
            "Epoch 1 Loss 0.5431\n",
            "Time taken for 1 epoch 824.3214650154114 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 0.3274\n",
            "Epoch 2 Loss 0.0002\n",
            "Time taken for 1 epoch 1.8706917762756348 sec\n",
            "\n",
            "Epoch 2 Batch 100 Loss 0.3436\n",
            "Epoch 2 Loss 0.0163\n",
            "Time taken for 1 epoch 46.32128667831421 sec\n",
            "\n",
            "Epoch 2 Batch 200 Loss 0.2784\n",
            "Epoch 2 Loss 0.0323\n",
            "Time taken for 1 epoch 90.800297498703 sec\n",
            "\n",
            "Epoch 2 Batch 300 Loss 0.2608\n",
            "Epoch 2 Loss 0.0478\n",
            "Time taken for 1 epoch 135.2457160949707 sec\n",
            "\n",
            "Epoch 2 Batch 400 Loss 0.2901\n",
            "Epoch 2 Loss 0.0629\n",
            "Time taken for 1 epoch 179.84463572502136 sec\n",
            "\n",
            "Epoch 2 Batch 500 Loss 0.2671\n",
            "Epoch 2 Loss 0.0778\n",
            "Time taken for 1 epoch 224.2554910182953 sec\n",
            "\n",
            "Epoch 2 Batch 600 Loss 0.2480\n",
            "Epoch 2 Loss 0.0926\n",
            "Time taken for 1 epoch 268.8192403316498 sec\n",
            "\n",
            "Epoch 2 Batch 700 Loss 0.2848\n",
            "Epoch 2 Loss 0.1071\n",
            "Time taken for 1 epoch 313.1810791492462 sec\n",
            "\n",
            "Epoch 2 Batch 800 Loss 0.2566\n",
            "Epoch 2 Loss 0.1210\n",
            "Time taken for 1 epoch 357.7401943206787 sec\n",
            "\n",
            "Epoch 2 Batch 900 Loss 0.2220\n",
            "Epoch 2 Loss 0.1348\n",
            "Time taken for 1 epoch 402.19976592063904 sec\n",
            "\n",
            "Epoch 2 Batch 1000 Loss 0.2226\n",
            "Epoch 2 Loss 0.1487\n",
            "Time taken for 1 epoch 447.00845217704773 sec\n",
            "\n",
            "Epoch 2 Batch 1100 Loss 0.2903\n",
            "Epoch 2 Loss 0.1623\n",
            "Time taken for 1 epoch 491.6012351512909 sec\n",
            "\n",
            "Epoch 2 Batch 1200 Loss 0.2629\n",
            "Epoch 2 Loss 0.1756\n",
            "Time taken for 1 epoch 536.334536075592 sec\n",
            "\n",
            "Epoch 2 Batch 1300 Loss 0.2215\n",
            "Epoch 2 Loss 0.1883\n",
            "Time taken for 1 epoch 581.0987229347229 sec\n",
            "\n",
            "Epoch 2 Batch 1400 Loss 0.2134\n",
            "Epoch 2 Loss 0.2010\n",
            "Time taken for 1 epoch 625.8838005065918 sec\n",
            "\n",
            "Epoch 2 Batch 1500 Loss 0.2466\n",
            "Epoch 2 Loss 0.2136\n",
            "Time taken for 1 epoch 670.5780897140503 sec\n",
            "\n",
            "Epoch 2 Batch 1600 Loss 0.2279\n",
            "Epoch 2 Loss 0.2262\n",
            "Time taken for 1 epoch 714.918053150177 sec\n",
            "\n",
            "Epoch 2 Batch 1700 Loss 0.2074\n",
            "Epoch 2 Loss 0.2385\n",
            "Time taken for 1 epoch 759.4425735473633 sec\n",
            "\n",
            "Epoch 2 Batch 1800 Loss 0.2400\n",
            "Epoch 2 Loss 0.2509\n",
            "Time taken for 1 epoch 803.88907289505 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 0.2151\n",
            "Epoch 3 Loss 0.0001\n",
            "Time taken for 1 epoch 0.5803375244140625 sec\n",
            "\n",
            "Epoch 3 Batch 100 Loss 0.1337\n",
            "Epoch 3 Loss 0.0094\n",
            "Time taken for 1 epoch 43.93784952163696 sec\n",
            "\n",
            "Epoch 3 Batch 200 Loss 0.1944\n",
            "Epoch 3 Loss 0.0185\n",
            "Time taken for 1 epoch 87.29007959365845 sec\n",
            "\n",
            "Epoch 3 Batch 300 Loss 0.1577\n",
            "Epoch 3 Loss 0.0277\n",
            "Time taken for 1 epoch 130.6340560913086 sec\n",
            "\n",
            "Epoch 3 Batch 400 Loss 0.1737\n",
            "Epoch 3 Loss 0.0366\n",
            "Time taken for 1 epoch 174.03640866279602 sec\n",
            "\n",
            "Epoch 3 Batch 500 Loss 0.1733\n",
            "Epoch 3 Loss 0.0455\n",
            "Time taken for 1 epoch 217.3829267024994 sec\n",
            "\n",
            "Epoch 3 Batch 600 Loss 0.1615\n",
            "Epoch 3 Loss 0.0545\n",
            "Time taken for 1 epoch 260.71583342552185 sec\n",
            "\n",
            "Epoch 3 Batch 700 Loss 0.1753\n",
            "Epoch 3 Loss 0.0641\n",
            "Time taken for 1 epoch 304.05270075798035 sec\n",
            "\n",
            "Epoch 3 Batch 800 Loss 0.1504\n",
            "Epoch 3 Loss 0.0734\n",
            "Time taken for 1 epoch 347.4985692501068 sec\n",
            "\n",
            "Epoch 3 Batch 900 Loss 0.1685\n",
            "Epoch 3 Loss 0.0826\n",
            "Time taken for 1 epoch 390.8976728916168 sec\n",
            "\n",
            "Epoch 3 Batch 1000 Loss 0.1370\n",
            "Epoch 3 Loss 0.0920\n",
            "Time taken for 1 epoch 434.2753257751465 sec\n",
            "\n",
            "Epoch 3 Batch 1100 Loss 0.1650\n",
            "Epoch 3 Loss 0.1010\n",
            "Time taken for 1 epoch 477.6174921989441 sec\n",
            "\n",
            "Epoch 3 Batch 1200 Loss 0.1401\n",
            "Epoch 3 Loss 0.1100\n",
            "Time taken for 1 epoch 520.8542907238007 sec\n",
            "\n",
            "Epoch 3 Batch 1300 Loss 0.2192\n",
            "Epoch 3 Loss 0.1193\n",
            "Time taken for 1 epoch 564.3368103504181 sec\n",
            "\n",
            "Epoch 3 Batch 1400 Loss 0.1759\n",
            "Epoch 3 Loss 0.1284\n",
            "Time taken for 1 epoch 607.7403018474579 sec\n",
            "\n",
            "Epoch 3 Batch 1500 Loss 0.1380\n",
            "Epoch 3 Loss 0.1375\n",
            "Time taken for 1 epoch 651.1827127933502 sec\n",
            "\n",
            "Epoch 3 Batch 1600 Loss 0.1777\n",
            "Epoch 3 Loss 0.1466\n",
            "Time taken for 1 epoch 694.5904369354248 sec\n",
            "\n",
            "Epoch 3 Batch 1700 Loss 0.1251\n",
            "Epoch 3 Loss 0.1557\n",
            "Time taken for 1 epoch 738.0261652469635 sec\n",
            "\n",
            "Epoch 3 Batch 1800 Loss 0.1750\n",
            "Epoch 3 Loss 0.1647\n",
            "Time taken for 1 epoch 781.4683425426483 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 0.1289\n",
            "Epoch 4 Loss 0.0001\n",
            "Time taken for 1 epoch 2.0588266849517822 sec\n",
            "\n",
            "Epoch 4 Batch 100 Loss 0.1095\n",
            "Epoch 4 Loss 0.0061\n",
            "Time taken for 1 epoch 46.74488425254822 sec\n",
            "\n",
            "Epoch 4 Batch 200 Loss 0.1265\n",
            "Epoch 4 Loss 0.0123\n",
            "Time taken for 1 epoch 91.19874954223633 sec\n",
            "\n",
            "Epoch 4 Batch 300 Loss 0.1442\n",
            "Epoch 4 Loss 0.0186\n",
            "Time taken for 1 epoch 135.7662901878357 sec\n",
            "\n",
            "Epoch 4 Batch 400 Loss 0.1377\n",
            "Epoch 4 Loss 0.0251\n",
            "Time taken for 1 epoch 180.2590000629425 sec\n",
            "\n",
            "Epoch 4 Batch 500 Loss 0.0902\n",
            "Epoch 4 Loss 0.0315\n",
            "Time taken for 1 epoch 224.91562724113464 sec\n",
            "\n",
            "Epoch 4 Batch 600 Loss 0.1154\n",
            "Epoch 4 Loss 0.0382\n",
            "Time taken for 1 epoch 269.35560154914856 sec\n",
            "\n",
            "Epoch 4 Batch 700 Loss 0.1103\n",
            "Epoch 4 Loss 0.0451\n",
            "Time taken for 1 epoch 313.7480821609497 sec\n",
            "\n",
            "Epoch 4 Batch 800 Loss 0.1716\n",
            "Epoch 4 Loss 0.0516\n",
            "Time taken for 1 epoch 358.36181116104126 sec\n",
            "\n",
            "Epoch 4 Batch 900 Loss 0.1353\n",
            "Epoch 4 Loss 0.0583\n",
            "Time taken for 1 epoch 402.9088017940521 sec\n",
            "\n",
            "Epoch 4 Batch 1000 Loss 0.1293\n",
            "Epoch 4 Loss 0.0648\n",
            "Time taken for 1 epoch 447.4442057609558 sec\n",
            "\n",
            "Epoch 4 Batch 1100 Loss 0.1087\n",
            "Epoch 4 Loss 0.0715\n",
            "Time taken for 1 epoch 492.237806558609 sec\n",
            "\n",
            "Epoch 4 Batch 1200 Loss 0.1324\n",
            "Epoch 4 Loss 0.0784\n",
            "Time taken for 1 epoch 536.8898048400879 sec\n",
            "\n",
            "Epoch 4 Batch 1300 Loss 0.1242\n",
            "Epoch 4 Loss 0.0852\n",
            "Time taken for 1 epoch 581.406774520874 sec\n",
            "\n",
            "Epoch 4 Batch 1400 Loss 0.1037\n",
            "Epoch 4 Loss 0.0921\n",
            "Time taken for 1 epoch 625.7273988723755 sec\n",
            "\n",
            "Epoch 4 Batch 1500 Loss 0.1214\n",
            "Epoch 4 Loss 0.0987\n",
            "Time taken for 1 epoch 670.1790418624878 sec\n",
            "\n",
            "Epoch 4 Batch 1600 Loss 0.1111\n",
            "Epoch 4 Loss 0.1055\n",
            "Time taken for 1 epoch 714.9036893844604 sec\n",
            "\n",
            "Epoch 4 Batch 1700 Loss 0.1097\n",
            "Epoch 4 Loss 0.1121\n",
            "Time taken for 1 epoch 759.4543855190277 sec\n",
            "\n",
            "Epoch 4 Batch 1800 Loss 0.1281\n",
            "Epoch 4 Loss 0.1189\n",
            "Time taken for 1 epoch 803.9905142784119 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 0.0828\n",
            "Epoch 5 Loss 0.0000\n",
            "Time taken for 1 epoch 0.5838508605957031 sec\n",
            "\n",
            "Epoch 5 Batch 100 Loss 0.0644\n",
            "Epoch 5 Loss 0.0044\n",
            "Time taken for 1 epoch 43.78905963897705 sec\n",
            "\n",
            "Epoch 5 Batch 200 Loss 0.1045\n",
            "Epoch 5 Loss 0.0091\n",
            "Time taken for 1 epoch 87.12240862846375 sec\n",
            "\n",
            "Epoch 5 Batch 300 Loss 0.0965\n",
            "Epoch 5 Loss 0.0141\n",
            "Time taken for 1 epoch 130.5594127178192 sec\n",
            "\n",
            "Epoch 5 Batch 400 Loss 0.0829\n",
            "Epoch 5 Loss 0.0192\n",
            "Time taken for 1 epoch 173.8621551990509 sec\n",
            "\n",
            "Epoch 5 Batch 500 Loss 0.1011\n",
            "Epoch 5 Loss 0.0242\n",
            "Time taken for 1 epoch 217.16277146339417 sec\n",
            "\n",
            "Epoch 5 Batch 600 Loss 0.0747\n",
            "Epoch 5 Loss 0.0290\n",
            "Time taken for 1 epoch 260.44676661491394 sec\n",
            "\n",
            "Epoch 5 Batch 700 Loss 0.0804\n",
            "Epoch 5 Loss 0.0341\n",
            "Time taken for 1 epoch 303.69924998283386 sec\n",
            "\n",
            "Epoch 5 Batch 800 Loss 0.1027\n",
            "Epoch 5 Loss 0.0391\n",
            "Time taken for 1 epoch 347.00001525878906 sec\n",
            "\n",
            "Epoch 5 Batch 900 Loss 0.0838\n",
            "Epoch 5 Loss 0.0441\n",
            "Time taken for 1 epoch 390.15276169776917 sec\n",
            "\n",
            "Epoch 5 Batch 1000 Loss 0.1320\n",
            "Epoch 5 Loss 0.0491\n",
            "Time taken for 1 epoch 433.30840706825256 sec\n",
            "\n",
            "Epoch 5 Batch 1100 Loss 0.0984\n",
            "Epoch 5 Loss 0.0543\n",
            "Time taken for 1 epoch 476.58370780944824 sec\n",
            "\n",
            "Epoch 5 Batch 1200 Loss 0.1154\n",
            "Epoch 5 Loss 0.0593\n",
            "Time taken for 1 epoch 519.673077583313 sec\n",
            "\n",
            "Epoch 5 Batch 1300 Loss 0.1214\n",
            "Epoch 5 Loss 0.0644\n",
            "Time taken for 1 epoch 562.9098436832428 sec\n",
            "\n",
            "Epoch 5 Batch 1400 Loss 0.0715\n",
            "Epoch 5 Loss 0.0695\n",
            "Time taken for 1 epoch 606.1456801891327 sec\n",
            "\n",
            "Epoch 5 Batch 1500 Loss 0.0897\n",
            "Epoch 5 Loss 0.0748\n",
            "Time taken for 1 epoch 649.3241336345673 sec\n",
            "\n",
            "Epoch 5 Batch 1600 Loss 0.0783\n",
            "Epoch 5 Loss 0.0800\n",
            "Time taken for 1 epoch 692.6483285427094 sec\n",
            "\n",
            "Epoch 5 Batch 1700 Loss 0.0767\n",
            "Epoch 5 Loss 0.0853\n",
            "Time taken for 1 epoch 736.0555114746094 sec\n",
            "\n",
            "Epoch 5 Batch 1800 Loss 0.0816\n",
            "Epoch 5 Loss 0.0908\n",
            "Time taken for 1 epoch 779.4184131622314 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 0.0814\n",
            "Epoch 6 Loss 0.0000\n",
            "Time taken for 1 epoch 1.7322030067443848 sec\n",
            "\n",
            "Epoch 6 Batch 100 Loss 0.0583\n",
            "Epoch 6 Loss 0.0036\n",
            "Time taken for 1 epoch 46.00575137138367 sec\n",
            "\n",
            "Epoch 6 Batch 200 Loss 0.0681\n",
            "Epoch 6 Loss 0.0072\n",
            "Time taken for 1 epoch 90.61212944984436 sec\n",
            "\n",
            "Epoch 6 Batch 300 Loss 0.0702\n",
            "Epoch 6 Loss 0.0110\n",
            "Time taken for 1 epoch 135.0054371356964 sec\n",
            "\n",
            "Epoch 6 Batch 400 Loss 0.0609\n",
            "Epoch 6 Loss 0.0147\n",
            "Time taken for 1 epoch 179.57278847694397 sec\n",
            "\n",
            "Epoch 6 Batch 500 Loss 0.0722\n",
            "Epoch 6 Loss 0.0185\n",
            "Time taken for 1 epoch 224.03264021873474 sec\n",
            "\n",
            "Epoch 6 Batch 600 Loss 0.0618\n",
            "Epoch 6 Loss 0.0223\n",
            "Time taken for 1 epoch 268.59266328811646 sec\n",
            "\n",
            "Epoch 6 Batch 700 Loss 0.0585\n",
            "Epoch 6 Loss 0.0262\n",
            "Time taken for 1 epoch 313.01937103271484 sec\n",
            "\n",
            "Epoch 6 Batch 800 Loss 0.0683\n",
            "Epoch 6 Loss 0.0300\n",
            "Time taken for 1 epoch 357.47393131256104 sec\n",
            "\n",
            "Epoch 6 Batch 900 Loss 0.0515\n",
            "Epoch 6 Loss 0.0339\n",
            "Time taken for 1 epoch 401.98527359962463 sec\n",
            "\n",
            "Epoch 6 Batch 1000 Loss 0.0838\n",
            "Epoch 6 Loss 0.0379\n",
            "Time taken for 1 epoch 446.4246506690979 sec\n",
            "\n",
            "Epoch 6 Batch 1100 Loss 0.0839\n",
            "Epoch 6 Loss 0.0420\n",
            "Time taken for 1 epoch 490.970666885376 sec\n",
            "\n",
            "Epoch 6 Batch 1200 Loss 0.0889\n",
            "Epoch 6 Loss 0.0462\n",
            "Time taken for 1 epoch 535.5025908946991 sec\n",
            "\n",
            "Epoch 6 Batch 1300 Loss 0.0816\n",
            "Epoch 6 Loss 0.0503\n",
            "Time taken for 1 epoch 579.7991302013397 sec\n",
            "\n",
            "Epoch 6 Batch 1400 Loss 0.0779\n",
            "Epoch 6 Loss 0.0546\n",
            "Time taken for 1 epoch 624.4695959091187 sec\n",
            "\n",
            "Epoch 6 Batch 1500 Loss 0.0748\n",
            "Epoch 6 Loss 0.0588\n",
            "Time taken for 1 epoch 669.1921660900116 sec\n",
            "\n",
            "Epoch 6 Batch 1600 Loss 0.0951\n",
            "Epoch 6 Loss 0.0629\n",
            "Time taken for 1 epoch 713.5763216018677 sec\n",
            "\n",
            "Epoch 6 Batch 1700 Loss 0.0671\n",
            "Epoch 6 Loss 0.0672\n",
            "Time taken for 1 epoch 758.1084904670715 sec\n",
            "\n",
            "Epoch 6 Batch 1800 Loss 0.0658\n",
            "Epoch 6 Loss 0.0716\n",
            "Time taken for 1 epoch 802.6915898323059 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 0.0570\n",
            "Epoch 7 Loss 0.0000\n",
            "Time taken for 1 epoch 0.588341236114502 sec\n",
            "\n",
            "Epoch 7 Batch 100 Loss 0.0618\n",
            "Epoch 7 Loss 0.0029\n",
            "Time taken for 1 epoch 43.80491805076599 sec\n",
            "\n",
            "Epoch 7 Batch 200 Loss 0.0491\n",
            "Epoch 7 Loss 0.0057\n",
            "Time taken for 1 epoch 87.15056204795837 sec\n",
            "\n",
            "Epoch 7 Batch 300 Loss 0.0632\n",
            "Epoch 7 Loss 0.0088\n",
            "Time taken for 1 epoch 130.53182697296143 sec\n",
            "\n",
            "Epoch 7 Batch 400 Loss 0.0403\n",
            "Epoch 7 Loss 0.0119\n",
            "Time taken for 1 epoch 173.79058504104614 sec\n",
            "\n",
            "Epoch 7 Batch 500 Loss 0.0651\n",
            "Epoch 7 Loss 0.0149\n",
            "Time taken for 1 epoch 217.05839228630066 sec\n",
            "\n",
            "Epoch 7 Batch 600 Loss 0.0529\n",
            "Epoch 7 Loss 0.0179\n",
            "Time taken for 1 epoch 260.236754655838 sec\n",
            "\n",
            "Epoch 7 Batch 700 Loss 0.0569\n",
            "Epoch 7 Loss 0.0209\n",
            "Time taken for 1 epoch 303.52306389808655 sec\n",
            "\n",
            "Epoch 7 Batch 800 Loss 0.0616\n",
            "Epoch 7 Loss 0.0241\n",
            "Time taken for 1 epoch 346.712361574173 sec\n",
            "\n",
            "Epoch 7 Batch 900 Loss 0.0638\n",
            "Epoch 7 Loss 0.0273\n",
            "Time taken for 1 epoch 389.9649465084076 sec\n",
            "\n",
            "Epoch 7 Batch 1000 Loss 0.0543\n",
            "Epoch 7 Loss 0.0305\n",
            "Time taken for 1 epoch 433.23081731796265 sec\n",
            "\n",
            "Epoch 7 Batch 1100 Loss 0.0613\n",
            "Epoch 7 Loss 0.0338\n",
            "Time taken for 1 epoch 476.593416929245 sec\n",
            "\n",
            "Epoch 7 Batch 1200 Loss 0.0584\n",
            "Epoch 7 Loss 0.0371\n",
            "Time taken for 1 epoch 519.8378612995148 sec\n",
            "\n",
            "Epoch 7 Batch 1300 Loss 0.0702\n",
            "Epoch 7 Loss 0.0405\n",
            "Time taken for 1 epoch 563.0984544754028 sec\n",
            "\n",
            "Epoch 7 Batch 1400 Loss 0.0663\n",
            "Epoch 7 Loss 0.0440\n",
            "Time taken for 1 epoch 606.4289946556091 sec\n",
            "\n",
            "Epoch 7 Batch 1500 Loss 0.0785\n",
            "Epoch 7 Loss 0.0474\n",
            "Time taken for 1 epoch 649.8545002937317 sec\n",
            "\n",
            "Epoch 7 Batch 1600 Loss 0.0459\n",
            "Epoch 7 Loss 0.0509\n",
            "Time taken for 1 epoch 693.1960453987122 sec\n",
            "\n",
            "Epoch 7 Batch 1700 Loss 0.0519\n",
            "Epoch 7 Loss 0.0546\n",
            "Time taken for 1 epoch 736.4475967884064 sec\n",
            "\n",
            "Epoch 7 Batch 1800 Loss 0.0544\n",
            "Epoch 7 Loss 0.0581\n",
            "Time taken for 1 epoch 779.7544085979462 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 0.0743\n",
            "Epoch 8 Loss 0.0000\n",
            "Time taken for 1 epoch 1.8660316467285156 sec\n",
            "\n",
            "Epoch 8 Batch 100 Loss 0.0399\n",
            "Epoch 8 Loss 0.0022\n",
            "Time taken for 1 epoch 46.34519171714783 sec\n",
            "\n",
            "Epoch 8 Batch 200 Loss 0.0429\n",
            "Epoch 8 Loss 0.0045\n",
            "Time taken for 1 epoch 90.76779532432556 sec\n",
            "\n",
            "Epoch 8 Batch 300 Loss 0.0573\n",
            "Epoch 8 Loss 0.0068\n",
            "Time taken for 1 epoch 135.27436137199402 sec\n",
            "\n",
            "Epoch 8 Batch 400 Loss 0.0777\n",
            "Epoch 8 Loss 0.0092\n",
            "Time taken for 1 epoch 179.97277522087097 sec\n",
            "\n",
            "Epoch 8 Batch 500 Loss 0.0315\n",
            "Epoch 8 Loss 0.0116\n",
            "Time taken for 1 epoch 224.4784960746765 sec\n",
            "\n",
            "Epoch 8 Batch 600 Loss 0.0401\n",
            "Epoch 8 Loss 0.0142\n",
            "Time taken for 1 epoch 269.0083005428314 sec\n",
            "\n",
            "Epoch 8 Batch 700 Loss 0.0427\n",
            "Epoch 8 Loss 0.0168\n",
            "Time taken for 1 epoch 313.70254015922546 sec\n",
            "\n",
            "Epoch 8 Batch 800 Loss 0.0660\n",
            "Epoch 8 Loss 0.0196\n",
            "Time taken for 1 epoch 358.01418232917786 sec\n",
            "\n",
            "Epoch 8 Batch 900 Loss 0.0513\n",
            "Epoch 8 Loss 0.0222\n",
            "Time taken for 1 epoch 402.4822335243225 sec\n",
            "\n",
            "Epoch 8 Batch 1000 Loss 0.0362\n",
            "Epoch 8 Loss 0.0249\n",
            "Time taken for 1 epoch 446.9625220298767 sec\n",
            "\n",
            "Epoch 8 Batch 1100 Loss 0.0514\n",
            "Epoch 8 Loss 0.0276\n",
            "Time taken for 1 epoch 491.66966366767883 sec\n",
            "\n",
            "Epoch 8 Batch 1200 Loss 0.0610\n",
            "Epoch 8 Loss 0.0302\n",
            "Time taken for 1 epoch 536.2212846279144 sec\n",
            "\n",
            "Epoch 8 Batch 1300 Loss 0.0553\n",
            "Epoch 8 Loss 0.0330\n",
            "Time taken for 1 epoch 580.8635220527649 sec\n",
            "\n",
            "Epoch 8 Batch 1400 Loss 0.0553\n",
            "Epoch 8 Loss 0.0360\n",
            "Time taken for 1 epoch 625.166960477829 sec\n",
            "\n",
            "Epoch 8 Batch 1500 Loss 0.0831\n",
            "Epoch 8 Loss 0.0391\n",
            "Time taken for 1 epoch 669.7603356838226 sec\n",
            "\n",
            "Epoch 8 Batch 1600 Loss 0.0715\n",
            "Epoch 8 Loss 0.0421\n",
            "Time taken for 1 epoch 714.3917663097382 sec\n",
            "\n",
            "Epoch 8 Batch 1700 Loss 0.0596\n",
            "Epoch 8 Loss 0.0453\n",
            "Time taken for 1 epoch 759.1261582374573 sec\n",
            "\n",
            "Epoch 8 Batch 1800 Loss 0.0570\n",
            "Epoch 8 Loss 0.0485\n",
            "Time taken for 1 epoch 803.3443510532379 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 0.0434\n",
            "Epoch 9 Loss 0.0000\n",
            "Time taken for 1 epoch 0.5698425769805908 sec\n",
            "\n",
            "Epoch 9 Batch 100 Loss 0.0406\n",
            "Epoch 9 Loss 0.0021\n",
            "Time taken for 1 epoch 43.986528635025024 sec\n",
            "\n",
            "Epoch 9 Batch 200 Loss 0.0538\n",
            "Epoch 9 Loss 0.0042\n",
            "Time taken for 1 epoch 87.28975772857666 sec\n",
            "\n",
            "Epoch 9 Batch 300 Loss 0.0360\n",
            "Epoch 9 Loss 0.0062\n",
            "Time taken for 1 epoch 130.61393547058105 sec\n",
            "\n",
            "Epoch 9 Batch 400 Loss 0.0328\n",
            "Epoch 9 Loss 0.0083\n",
            "Time taken for 1 epoch 173.8164026737213 sec\n",
            "\n",
            "Epoch 9 Batch 500 Loss 0.0401\n",
            "Epoch 9 Loss 0.0105\n",
            "Time taken for 1 epoch 217.14408159255981 sec\n",
            "\n",
            "Epoch 9 Batch 600 Loss 0.0468\n",
            "Epoch 9 Loss 0.0126\n",
            "Time taken for 1 epoch 260.5394723415375 sec\n",
            "\n",
            "Epoch 9 Batch 700 Loss 0.0416\n",
            "Epoch 9 Loss 0.0147\n",
            "Time taken for 1 epoch 303.8963129520416 sec\n",
            "\n",
            "Epoch 9 Batch 800 Loss 0.0446\n",
            "Epoch 9 Loss 0.0170\n",
            "Time taken for 1 epoch 347.0750389099121 sec\n",
            "\n",
            "Epoch 9 Batch 900 Loss 0.0321\n",
            "Epoch 9 Loss 0.0193\n",
            "Time taken for 1 epoch 390.29736375808716 sec\n",
            "\n",
            "Epoch 9 Batch 1000 Loss 0.0415\n",
            "Epoch 9 Loss 0.0215\n",
            "Time taken for 1 epoch 433.4454095363617 sec\n",
            "\n",
            "Epoch 9 Batch 1100 Loss 0.0373\n",
            "Epoch 9 Loss 0.0238\n",
            "Time taken for 1 epoch 476.64062237739563 sec\n",
            "\n",
            "Epoch 9 Batch 1200 Loss 0.0451\n",
            "Epoch 9 Loss 0.0262\n",
            "Time taken for 1 epoch 519.7199850082397 sec\n",
            "\n",
            "Epoch 9 Batch 1300 Loss 0.0423\n",
            "Epoch 9 Loss 0.0285\n",
            "Time taken for 1 epoch 562.9971988201141 sec\n",
            "\n",
            "Epoch 9 Batch 1400 Loss 0.0382\n",
            "Epoch 9 Loss 0.0311\n",
            "Time taken for 1 epoch 606.1412417888641 sec\n",
            "\n",
            "Epoch 9 Batch 1500 Loss 0.0616\n",
            "Epoch 9 Loss 0.0335\n",
            "Time taken for 1 epoch 649.4229867458344 sec\n",
            "\n",
            "Epoch 9 Batch 1600 Loss 0.0502\n",
            "Epoch 9 Loss 0.0360\n",
            "Time taken for 1 epoch 692.7339794635773 sec\n",
            "\n",
            "Epoch 9 Batch 1700 Loss 0.0510\n",
            "Epoch 9 Loss 0.0387\n",
            "Time taken for 1 epoch 736.0140314102173 sec\n",
            "\n",
            "Epoch 9 Batch 1800 Loss 0.0432\n",
            "Epoch 9 Loss 0.0414\n",
            "Time taken for 1 epoch 779.2699751853943 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 0.0398\n",
            "Epoch 10 Loss 0.0000\n",
            "Time taken for 1 epoch 1.8990700244903564 sec\n",
            "\n",
            "Epoch 10 Batch 100 Loss 0.0289\n",
            "Epoch 10 Loss 0.0017\n",
            "Time taken for 1 epoch 46.413491010665894 sec\n",
            "\n",
            "Epoch 10 Batch 200 Loss 0.0331\n",
            "Epoch 10 Loss 0.0033\n",
            "Time taken for 1 epoch 90.95965719223022 sec\n",
            "\n",
            "Epoch 10 Batch 300 Loss 0.0396\n",
            "Epoch 10 Loss 0.0049\n",
            "Time taken for 1 epoch 135.52084732055664 sec\n",
            "\n",
            "Epoch 10 Batch 400 Loss 0.0327\n",
            "Epoch 10 Loss 0.0066\n",
            "Time taken for 1 epoch 180.11229419708252 sec\n",
            "\n",
            "Epoch 10 Batch 500 Loss 0.0289\n",
            "Epoch 10 Loss 0.0084\n",
            "Time taken for 1 epoch 224.88466572761536 sec\n",
            "\n",
            "Epoch 10 Batch 600 Loss 0.0326\n",
            "Epoch 10 Loss 0.0101\n",
            "Time taken for 1 epoch 269.1240870952606 sec\n",
            "\n",
            "Epoch 10 Batch 700 Loss 0.0373\n",
            "Epoch 10 Loss 0.0119\n",
            "Time taken for 1 epoch 313.60071897506714 sec\n",
            "\n",
            "Epoch 10 Batch 800 Loss 0.0258\n",
            "Epoch 10 Loss 0.0139\n",
            "Time taken for 1 epoch 358.1505796909332 sec\n",
            "\n",
            "Epoch 10 Batch 900 Loss 0.0332\n",
            "Epoch 10 Loss 0.0158\n",
            "Time taken for 1 epoch 402.92907524108887 sec\n",
            "\n",
            "Epoch 10 Batch 1000 Loss 0.0449\n",
            "Epoch 10 Loss 0.0177\n",
            "Time taken for 1 epoch 447.5522999763489 sec\n",
            "\n",
            "Epoch 10 Batch 1100 Loss 0.0321\n",
            "Epoch 10 Loss 0.0197\n",
            "Time taken for 1 epoch 492.1331913471222 sec\n",
            "\n",
            "Epoch 10 Batch 1200 Loss 0.0479\n",
            "Epoch 10 Loss 0.0218\n",
            "Time taken for 1 epoch 536.8676142692566 sec\n",
            "\n",
            "Epoch 10 Batch 1300 Loss 0.0340\n",
            "Epoch 10 Loss 0.0241\n",
            "Time taken for 1 epoch 581.1164448261261 sec\n",
            "\n",
            "Epoch 10 Batch 1400 Loss 0.0417\n",
            "Epoch 10 Loss 0.0263\n",
            "Time taken for 1 epoch 625.8098340034485 sec\n",
            "\n",
            "Epoch 10 Batch 1500 Loss 0.0422\n",
            "Epoch 10 Loss 0.0286\n",
            "Time taken for 1 epoch 670.4033234119415 sec\n",
            "\n",
            "Epoch 10 Batch 1600 Loss 0.0425\n",
            "Epoch 10 Loss 0.0309\n",
            "Time taken for 1 epoch 714.8079755306244 sec\n",
            "\n",
            "Epoch 10 Batch 1700 Loss 0.0388\n",
            "Epoch 10 Loss 0.0331\n",
            "Time taken for 1 epoch 759.255298614502 sec\n",
            "\n",
            "Epoch 10 Batch 1800 Loss 0.0336\n",
            "Epoch 10 Loss 0.0354\n",
            "Time taken for 1 epoch 803.7870721817017 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHlMkiug4SNB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3e8eac09-fce8-4333-f524-091342351b24"
      },
      "source": [
        "# RESTORE THE LAST CHECKPOINT\n",
        "os.listdir(checkpoints_dir)\n",
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir=checkpoints_dir))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f8fb5279e80>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXpg-aMdvklZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# PREDICTION\n",
        "def evaluate(sentence):\n",
        "  sentence = preprocess_sentence(sentence)\n",
        "  inputs = [inp_word_index.word_index[i] for i in sentence.split(' ')]\n",
        "  inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "      [inputs],\n",
        "      padding='post',\n",
        "      maxlen=input_tensor_max_len)\n",
        "  inputs = tf.convert_to_tensor(inputs)\n",
        "  \n",
        "  # results\n",
        "  result = ''\n",
        "\n",
        "  hidden = [tf.zeros((1,units))]\n",
        "  enc_out, enc_hidden = encoder(inputs,hidden)\n",
        "\n",
        "  dec_hidden = enc_hidden\n",
        "  dec_input = tf.expand_dims([targ_word_index.word_index['<start>']],0)\n",
        "\n",
        "  for t in range(targ_tensor_max_len):\n",
        "    predictions,dec_hidden, attention_weights = decoder(\n",
        "        dec_input,\n",
        "        dec_hidden,\n",
        "        enc_out\n",
        "    )\n",
        "    predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "    print('Predicted ID:',predicted_id)\n",
        "    result += targ_word_index.index_word[predicted_id]+' '\n",
        "    if(targ_word_index.index_word[predicted_id]=='<end>'):\n",
        "      return result, sentence\n",
        "    \n",
        "    # THe predicted ID is fed back into the modedl\n",
        "    dec_input = tf.expand_dims([predicted_id],0)\n",
        "  return result, sentence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPmMyYkqvk7g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def translate(sentence):\n",
        "  result, sentence = evaluate(sentence)\n",
        "\n",
        "  print('Input: ',sentence)\n",
        "  print('Predicted Translation: ', result)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEA93FeO9Iyj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "f5e3217b-3adb-4af2-bfd3-a7912047c681"
      },
      "source": [
        "# TRANSLATE\n",
        "translate(u'hace mucho frio aqui.')\n"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicted ID: 14\n",
            "Predicted ID: 15\n",
            "Predicted ID: 59\n",
            "Predicted ID: 273\n",
            "Predicted ID: 62\n",
            "Predicted ID: 3\n",
            "Predicted ID: 2\n",
            "Input:  <start> hace mucho frio aqui . <end>\n",
            "Predicted Translation:  it s very cold here . <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvTeI9AQ-2bl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "21e6046a-ef88-48c3-c024-8b2dd8e2ef83"
      },
      "source": [
        "translate(u'esta es mi vida.')\n"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicted ID: 23\n",
            "Predicted ID: 11\n",
            "Predicted ID: 25\n",
            "Predicted ID: 203\n",
            "Predicted ID: 3\n",
            "Predicted ID: 2\n",
            "Input:  <start> esta es mi vida . <end>\n",
            "Predicted Translation:  this is my life . <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16Q0yeB--6Ci",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "5685fc88-9ba1-4b27-d516-6a12c370d839"
      },
      "source": [
        "translate(u'¿todavia estan en casa?')\n"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicted ID: 31\n",
            "Predicted ID: 7\n",
            "Predicted ID: 160\n",
            "Predicted ID: 44\n",
            "Predicted ID: 115\n",
            "Predicted ID: 10\n",
            "Predicted ID: 2\n",
            "Input:  <start> ¿ todavia estan en casa ? <end>\n",
            "Predicted Translation:  are you still at home ? <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AgOdrge--98O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "4803a184-31a7-4d63-c7bb-4b75cb30787b"
      },
      "source": [
        "translate('¿todavia estan en casa?')\n"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicted ID: 31\n",
            "Predicted ID: 7\n",
            "Predicted ID: 160\n",
            "Predicted ID: 44\n",
            "Predicted ID: 115\n",
            "Predicted ID: 10\n",
            "Predicted ID: 2\n",
            "Input:  <start> ¿ todavia estan en casa ? <end>\n",
            "Predicted Translation:  are you still at home ? <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odANtHW4_Abt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "ec9bafb7-f6b1-4cee-d3db-f16eb2342005"
      },
      "source": [
        "translate(u'mi amigo')\n"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicted ID: 25\n",
            "Predicted ID: 229\n",
            "Predicted ID: 3\n",
            "Predicted ID: 2\n",
            "Input:  <start> mi amigo <end>\n",
            "Predicted Translation:  my friend . <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OIHztb0v94di",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2731c35e-b420-4a1f-89ee-2b658aa26d8f"
      },
      "source": [
        "targ_word_index.index_word[14]"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'it'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    }
  ]
}