{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TextGeneration2.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Georgemburu/MACHINE-LEARNING/blob/master/TextGeneration2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etWi9d4q99Q7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "outputId": "18dea958-514c-4c3d-8df8-1019ac8edc9b"
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import tensorflow as tf\n",
        "tf.enable_eager_execution()\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQWuqcvu_uS3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmU9eSRy-EHq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "cb886e95-45f7-44a7-f139-5303068510be"
      },
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1122304/1115394 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_QXERtOg-Ixn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "39f60bce-efd0-4d5e-cb64-cd05f12e155f"
      },
      "source": [
        "# Read the data\n",
        "text = open(path_to_file,'rb').read().decode(encoding='utf-8')\n",
        "print('text len', len(text))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "text len 1115394\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctMRzUiz-JHR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "de946bc0-5de9-433f-d5f4-fe3197db0a94"
      },
      "source": [
        "# The unique chars in the file\n",
        "vocab = sorted(set(text))\n",
        "print('Num of unique chars: ',len(vocab))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Num of unique chars:  65\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6osMsx-Z-JXj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Vectorize the text\n",
        "char2idx = {u:i for i,u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8SXqM96-nTl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text_as_int = np.array([char2idx[c] for c in text])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLD6MBqv-nN7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8c3c9214-805a-47e1-be6c-3348bf6ce301"
      },
      "source": [
        "text_as_int"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([18, 47, 56, ..., 45,  8,  0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azXvPyMv-nGe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "c6160a56-8b40-4720-e9ba-5901c2fba5ff"
      },
      "source": [
        "seq_length = 100\n",
        "examples_per_epoch = len(text)\n",
        "\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "\n",
        "for i in char_dataset.take(5):\n",
        "  print(idx2char[i.numpy()])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkrCMHE6_4hC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "fa680f03-9179-4031-a0c8-f2455b739263"
      },
      "source": [
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for item in sequences.take(1):\n",
        "  txt = repr(''.join(idx2char[item.numpy()]))\n",
        "  print(txt)\n",
        "  print(len(txt))\n",
        "  print(len(item.numpy()))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "110\n",
            "101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7trVAy54_43t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_input_target(chunk):\n",
        "  input_text = chunk[:-1]\n",
        "  target_text = chunk[1:]\n",
        "  return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0GUgA4xQBCKo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "d5e904f6-66d8-44c1-e4e4-02072ff1f10d"
      },
      "source": [
        "for input_example, target_example in dataset.take(1):\n",
        "  print('Input->',repr(''.join(idx2char[input_example.numpy()])))\n",
        "  print('Target->', repr(''.join(idx2char[target_example.numpy()])))\n",
        "  "
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input-> 'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target-> 'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHChsNtaBB-a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "outputId": "062950f0-ec54-4e8c-e9db-753b0c934173"
      },
      "source": [
        "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
        "    print(\"Step {:4d}\".format(i))\n",
        "    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
        "    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step    0\n",
            "  input: 18 ('F')\n",
            "  expected output: 47 ('i')\n",
            "Step    1\n",
            "  input: 47 ('i')\n",
            "  expected output: 56 ('r')\n",
            "Step    2\n",
            "  input: 56 ('r')\n",
            "  expected output: 57 ('s')\n",
            "Step    3\n",
            "  input: 57 ('s')\n",
            "  expected output: 58 ('t')\n",
            "Step    4\n",
            "  input: 58 ('t')\n",
            "  expected output: 1 (' ')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5C1aFTbCT4u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# CREATE  TRAINIG BATCHES"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3nwwc5m_5Bf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "2e578e7e-6689-47dd-cb05-f49b88096515"
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "dataset"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<DatasetV1Adapter shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxRQEn1NDg_i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# BUILD THE MODEL"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5E4r3JrPDg0d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_size = len(vocab)\n",
        "embedding_dim = 256\n",
        "rnn_units = 1024"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SkEI9q9KDree",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                              batch_input_shape=[batch_size, None]),\n",
        "    tf.keras.layers.GRU(rnn_units,\n",
        "                        return_sequences=True,\n",
        "                        stateful=True,\n",
        "                        recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.Dense(vocab_size)                                  \n",
        "  ])\n",
        "  return model\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VejnnW-QDrVS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_model(\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units,\n",
        "    batch_size=BATCH_SIZE\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62Xlkc2KE8Pz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        },
        "outputId": "43cfdfd4-6367-49d4-bda8-813a6ecf80ca"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (64, None, 256)           16640     \n",
            "_________________________________________________________________\n",
            "gru (GRU)                    (64, None, 1024)          3935232   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (64, None, 65)            66625     \n",
            "=================================================================\n",
            "Total params: 4,018,497\n",
            "Trainable params: 4,018,497\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Ib1StA5FjuZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "816d749f-02af-4f2f-9b29-908478e05ada"
      },
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "  example_batch_predictions = model(input_example_batch)\n",
        "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 100, 65) # (batch_size, sequence_length, vocab_size)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwWWWdrqGbf4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "f2134519-930a-430c-8dac-a30216e5d95f"
      },
      "source": [
        "# Sample from distribution\n",
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()\n",
        "sampled_indices"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([41, 53, 35, 59, 41, 19, 56, 20, 21,  1, 16, 31, 11, 36, 44, 46, 35,\n",
              "       36, 61, 38, 45, 14, 58, 53, 18, 23, 17, 19, 11, 42, 38, 62, 12, 18,\n",
              "       29,  4, 61, 52, 52, 56, 35,  3, 33, 29, 13, 11, 23, 18, 46,  5, 55,\n",
              "       36, 40, 41, 52, 54, 10,  9, 28, 61, 50, 21, 35, 59,  1, 36, 54,  5,\n",
              "       39, 25, 58, 16, 31,  1,  1,  9, 29, 12, 18, 39,  3, 41, 60, 31, 46,\n",
              "        2, 35, 18, 26, 30, 17, 21, 52, 35, 37, 62, 50, 33, 60, 33])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqGiUQQiGrKa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "ba7029d7-c461-467c-bbbb-c98030e7bcd8"
      },
      "source": [
        "print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\n",
        "print()\n",
        "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices ])))\n"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: \n",
            " 'r never heard a play--\\nYou break into some merry passion\\nAnd so offend him; for I tell you, sirs,\\nIf'\n",
            "\n",
            "Next Char Predictions: \n",
            " \"coWucGrHI DS;XfhWXwZgBtoFKEG;dZx?FQ&wnnrW$UQA;KFh'qXbcnp:3PwlIWu Xp'aMtDS  3Q?Fa$cvSh!WFNREInWYxlUvU\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4oS4tDpHLno",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TRAIN THE MODEL"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEA6TeXXHlNy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "81514c58-9127-4863-d6bf-7b44785f30e5"
      },
      "source": [
        "def loss(labels,logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels,\n",
        "                                                         logits,\n",
        "                                                         from_logits=True)\n",
        "  \n",
        "example_batch_loss = loss(target_example_batch, example_batch_predictions)\n",
        "print('Prediction shape:', example_batch_predictions.shape,\" # (batch_size, sequence_length, vocab_size)\")\n",
        "print('scalar_loss:  ', example_batch_loss.numpy().mean())"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction shape: (64, 100, 65)  # (batch_size, sequence_length, vocab_size)\n",
            "scalar_loss:   4.175373\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0IyfKOsI6AI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "988JHu16I6c_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Configure the checkpoints\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt_{epoch}')\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_AKipBguKDpM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Execute the training"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ihjoGzK7J3ft",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "outputId": "dc937135-dfb1-4443-fce5-e0fba092bf5a"
      },
      "source": [
        "EPOCHS = 10\n",
        "history = model.fit(dataset,epochs=EPOCHS,callbacks=[checkpoint_callback])"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "172/172 [==============================] - 40s 233ms/step - loss: 2.7189\n",
            "Epoch 2/10\n",
            "172/172 [==============================] - 38s 224ms/step - loss: 1.9856\n",
            "Epoch 3/10\n",
            "172/172 [==============================] - 38s 222ms/step - loss: 1.7204\n",
            "Epoch 4/10\n",
            "172/172 [==============================] - 38s 223ms/step - loss: 1.5684\n",
            "Epoch 5/10\n",
            "172/172 [==============================] - 39s 225ms/step - loss: 1.4736\n",
            "Epoch 6/10\n",
            "172/172 [==============================] - 38s 224ms/step - loss: 1.4064\n",
            "Epoch 7/10\n",
            "172/172 [==============================] - 38s 224ms/step - loss: 1.3531\n",
            "Epoch 8/10\n",
            "172/172 [==============================] - 38s 223ms/step - loss: 1.3064\n",
            "Epoch 9/10\n",
            "172/172 [==============================] - 39s 226ms/step - loss: 1.2610\n",
            "Epoch 10/10\n",
            "172/172 [==============================] - 39s 228ms/step - loss: 1.2156\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XerkxqfOKHDP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# GENERATE TEXT"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JwP2oNRnKG7B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f4831c3c-9ee2-49df-a704-ec4245475aa0"
      },
      "source": [
        "# Restore the last checkpoint\n",
        "tf.train.latest_checkpoint(checkpoint_dir)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'./training_checkpoints/ckpt_10'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MwSUpEsfKtVz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        },
        "outputId": "7af0dedf-45a7-42f3-d851-26f632fb0383"
      },
      "source": [
        "model = build_model(vocab_size, embedding_dim, rnn_units,batch_size=1)\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "model.build(tf.TensorShape([1,None]))\n",
        "model.summary()"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (1, None, 256)            16640     \n",
            "_________________________________________________________________\n",
            "gru_2 (GRU)                  (1, None, 1024)           3935232   \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (1, None, 65)             66625     \n",
            "=================================================================\n",
            "Total params: 4,018,497\n",
            "Trainable params: 4,018,497\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vVns8CtKtuE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_text(model, start_string):\n",
        "  # Evaluation step (generating text using the learned model)\n",
        "\n",
        "  # Number of characters to generate\n",
        "  num_generate = 1000\n",
        "\n",
        "  # Converting our start string to numbers (vectorizing)\n",
        "  input_eval = [char2idx[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  # Empty string to store our results\n",
        "  text_generated = []\n",
        "\n",
        "  # Low temperatures results in more predictable text.\n",
        "  # Higher temperatures results in more surprising text.\n",
        "  # Experiment to find the best setting.\n",
        "  temperature = 1.0\n",
        "\n",
        "  # Here batch size == 1\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "      predictions = model(input_eval)\n",
        "      # remove the batch dimension\n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "      # using a categorical distribution to predict the word returned by the model\n",
        "      predictions = predictions / temperature\n",
        "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "      # We pass the predicted word as the next input to the model\n",
        "      # along with the previous hidden state\n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "      text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ez56t8SIQu7h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 729
        },
        "outputId": "85489096-d05f-4e94-fa82-b02fe5971a70"
      },
      "source": [
        "print(generate_text(model, start_string=u'ROMEO: '))"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ROMEO: I do to be his person law her have shows\n",
            "woe gone.\n",
            "\n",
            "Shepherd:\n",
            "Marry, I can never tate all grace.\n",
            "\n",
            "HORTENSIO:\n",
            "Cunst thou that dry:\n",
            "I am they please.\n",
            "\n",
            "DUCHESS OF YORCK:\n",
            "All thus confession: but thou dumst her,\n",
            "Prived in the week, and not\n",
            "My babour hears slain:\n",
            "NIS:\n",
            "What dost thou better it, with grace of daughter:\n",
            "No daughter to the Thursday name; or else\n",
            "the grace's day that to drumple\n",
            "You have done maiden.\n",
            "See the duke, or if now, 'twas Vine\n",
            "Of all the villain face that Egardomeness\n",
            "That have goed waning their worst: Carreyst,\n",
            "You whom thee take fur gallows?\n",
            "\n",
            "ARIEL:\n",
            "Alack, down with our house!\n",
            "\n",
            "ARCHUCKIO:\n",
            "Nay, go you kn will, For do\n",
            "Is sword.\n",
            "\n",
            "HENRY BOLINGBROKE:\n",
            "Good for, and may slee it but your might hand what would\n",
            "IGHARD IV:\n",
            "Ad it may be\n",
            "Now will this by the way how to unstand in overroy's tear,\n",
            "From which with her our fair queen is commonness\n",
            "Abaur her, or sure I have a dream obsearth,\n",
            "That where he let him grow: out of the liston charity:\n",
            "Her 'Gon, I would have framed or I:\n",
            "I kno\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}